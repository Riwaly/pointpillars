# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/10_train.ipynb (unless otherwise specified).

__all__ = ['ident_time', 'logger', 'log_format', 'log_formatter', 'log_handler', 'run_folder', 'writer', 'validate',
           'train']

# Cell
import logging
import torch
from torch import autograd
from torch.utils.tensorboard import SummaryWriter
import datetime

from .utils.io import read_config, save_network_checkpoint, save_network
from .data.dataset import VelTrainDataset, collate_fn, OverfitSampler
from .modules.pointpillars import PointPillars, init_weights
from .loss import PointPillarsLoss


# Cell
# time to identify the run on folders and checkpoints etc
ident_time = datetime.datetime.now().strftime("%d-%m-%Y-%H-%M-%S")

# Cell
# logging
logger = logging.getLogger()
logger.setLevel(logging.DEBUG)

log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
log_formatter = logging.Formatter(log_format)

log_handler = logging.FileHandler("/home/qhs67/git/bachelorthesis_sven_thaele/code/pointpillars.log", mode='w')
log_handler.setFormatter(log_formatter)
logger.addHandler(log_handler)

# Cell
# tensorboard writer
run_folder = "/home/qhs67/git/bachelorthesis_sven_thaele/code/runs/{}/".format(ident_time)

writer = SummaryWriter(run_folder)

# Cell
def _train_setup():
    """

    """
    batch_size = 3
    init_lr = 2 * 10**-4
    #init_lr = 1 * 10**-4

    logger.info("Start network training..")
    torch.cuda.empty_cache()
    torch.multiprocessing.set_start_method('spawn')

    # TODO: move to config file
    conf = read_config()
    vel_folder = "/home/qhs67/git/bachelorthesis_sven_thaele/code/data/kitti/training/velodyne/training"
    label_folder ="/home/qhs67/git/bachelorthesis_sven_thaele/code/data/kitti/training/label_2/training"

    ds_train = VelTrainDataset(vel_folder, label_folder)
    """dl_train = torch.utils.data.DataLoader(ds_train,
                                           batch_size=batch_size,
                                           num_workers=1,
                                           collate_fn=collate_fn,
                                           shuffle=True)"""

    sampler = OverfitSampler(ds_train, batch_size, nb_samples=20, shuffle=True)
    for i in sampler:
        print(i)

    dl_train = torch.utils.data.DataLoader(ds_train,
                                           batch_size=batch_size,
                                           sampler=sampler,
                                           num_workers=0,
                                           collate_fn=collate_fn)

    # modules
    pointpillars = PointPillars(conf)
    loss_func = PointPillarsLoss()
    pointpillars.train()
    loss_func.train()

    # TODO: also init bias?
    #pointpillars.apply(init_weights)
    # move to gpu
    pointpillars.cuda()
    loss_func.cuda()

    optimizer = torch.optim.Adam(pointpillars.parameters(), lr=init_lr)
    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 15, gamma=0.8, last_epoch=-1)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 2000, gamma=0.8, last_epoch=-1)

    return pointpillars, loss_func, optimizer, scheduler, dl_train

# Cell
def _train_step(batch: torch.Tensor,
               pointpillars: torch.nn.Module,
               loss_func: torch.nn.Module,
               optimizer: torch.optim.Adam,
               epoch: int,
               i: int) -> torch.nn.Module:

    """
    Performs a training step
    """

    pil_batch, ind_batch, label_batch, label_mask = batch

    # -> forward pass through network
    preds = pointpillars(pil_batch, ind_batch, label_batch, label_mask)

    loss = loss_func(preds, writer, epoch, i)
    loss.backward()
    optimizer.step()

    del pil_batch, ind_batch, label_batch, preds

    return loss

# Cell
def validate(network: torch.nn.Module, loss_func: torch.nn.Module, epoch, nbr_val_batches: int = 300):
    """
    Validates the network on the loss function on the validation dataset
    """
    batch_size = 3

    validation_folder = "/home/qhs67/git/bachelorthesis_sven_thaele/code/data/kitti/training/velodyne/validation"
    label_folder ="/home/qhs67/git/bachelorthesis_sven_thaele/code/data/kitti/training/label_2/validation"

    with torch.no_grad():
        ds_val = VelTrainDataset(validation_folder, label_folder)
        dl_val = torch.utils.data.DataLoader(ds_val,
                                           batch_size=batch_size,
                                           num_workers=2,
                                           collate_fn=collate_fn,
                                           shuffle=True)

        running_val_loss = 0

        for i, batch in enumerate(dl_val):
            # stop after given number of data
            if i >= nbr_val_batches:
                break

            pil_batch, ind_batch, label_batch, label_mask = batch
            preds = network(pil_batch, ind_batch, label_batch, label_mask)
            loss = loss_func(preds)

            running_val_loss += loss.item()

            print("Val Epoch: {}, Batch {} with Loss {} and running Loss {}.".format(epoch, i, loss.item(), running_val_loss/(i+1)))

            #writer.add_scalar("Epoch {}/Validation Loss".format(epoch), running_val_loss/(i+1), i)
            #writer.flush()

        writer.add_scalar("Epochs/Validation Loss", running_val_loss/nbr_val_batches, epoch)
        writer.flush()


# Cell
def train(val: bool = False, save_nw: bool = False):
    """

    :param val: bool if validation should be used
    :param save_nw: bool if network state should be saved after training
    """
    n_epochs = 10000


    pointpillars, loss_func, optimizer, scheduler, dl_train = _train_setup()

    try:
        for epoch in range(n_epochs):
            running_loss = 0
            for i, batch in enumerate(dl_train):


                optimizer.zero_grad()
                loss = _train_step(batch, pointpillars, loss_func, optimizer, epoch, i)
                running_loss += loss.item()

                logger.debug("Epoch: {}, Batch {} with Loss {} and running Loss {}.".format(epoch, i, loss.item(), running_loss/(i+1)))
                print("Epoch: {}, Batch {} with Loss {} and running Loss {}.".format(epoch, i, loss.item(), running_loss/(i+1)))

                torch.cuda.empty_cache()
                writer.add_scalar("Epoch {}/Running Loss".format(epoch), running_loss/(i+1), i)
                writer.flush()

            # after epoch
            scheduler.step()
            writer.add_scalar("Epochs/Running Loss", running_loss/len(dl_train), epoch)
            writer.add_scalar("Epochs/Learning Rate", scheduler.get_last_lr()[0], epoch)
            writer.flush()

            if val:
                validate(pointpillars, loss_func, epoch, nbr_val_batches=10)

        # after training
        print('Finished Training')

    except Exception as e:
        print(e)
        logger.exception("An exception occured")
        save_network_checkpoint(pointpillars, optimizer, scheduler, loss, ident_time, epoch)
        exit()

    if save_nw:
        # save network to location from config
        save_network(pointpillars, ident_time, n_epochs)


    writer.close()

# Cell
if __name__ == '__main__':
    train(val=False, save_nw=True)



